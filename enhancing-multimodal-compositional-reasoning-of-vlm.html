<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining">
    <meta name="keywords" content="Generative, Negative, Mining, WACV">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://github.com/ugorsahin">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>
            </div>
        </div>
    </nav>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://github.com/ugorsahin">Uğur Şahin</a><sup>*1</sup>,</span>
                            <span class="author-block">
                                <a href="https://hangligit.github.io">Hang Li</a><sup>*2, 4</sup>,</span>
                            <span class="author-block">
                                <a href="https://cvg.cit.tum.de/members/khamuham">Qadeer Khan</a><sup>1,3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://cvg.cit.tum.de/members/cremers">Daniel Cremers</a><sup>1,3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a><sup>2,3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Technical University of Munich,</span>
                            <span class="author-block"><sup>2</sup>Ludwig Maximilian University of Munich</span>
                            <span class="author-block"><sup>3</sup>Munich Center for Machine Learning (MCML)</span>
                            <span class="author-block"><sup>4</sup>Siemens AG</span>
                            <br>
                            <span class="author-block" style="font-size:18px"><sup>*</sup>Equal contribution</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2311.03964"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/ugorsahin/Generative-Negative-Mining"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://huggingface.co/datasets/ugursahin/Generative-Negative-Mining-Dataset"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="far fa-images"></i>
                                        </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="./static/generative-negative-mining-poster.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-print"></i>
                                        </span>
                                        <span>Poster</span> 
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>Contemporary large-scale visual language models
                        (VLMs) exhibit strong representation capacities, making
                        them ubiquitous for enhancing the image and text under-
                        standing tasks.</p>
                        
                        <p>They are often trained in a contrastive
                        manner on a large and diverse corpus of images and cor-
                        responding text captions scraped from the internet. De-
                        spite this, VLMs often struggle with compositional reason-
                        ing tasks which require a fine-grained understanding of the
                        complex interactions of objects and their attributes.
                        </p>
                        
                        <p></p> This failure can be attributed to two main factors: 1) Contrastive
                        approaches have traditionally focused on mining negative
                        examples from existing datasets. However, the mined nega-
                        tive examples might not be difficult for the model to discrim-
                        inate from the positive. An alternative to mining would be
                        negative sample generation 2) But existing generative ap-
                        proaches primarily focus on generating hard negative texts
                        associated with a given image. Mining in the other di-
                        rection, i.e., generating negative image samples associated
                        with a given text has been ignored.</p>
                        <p> To overcome both these limitations, we propose a framework that not only mines
                        in both directions but also generates challenging negative
                        samples in both modalities, i.e., images and texts. Leverag-
                        ing these generative hard negative samples, we significantly
                        enhance VLMs’ performance in tasks involving multimodal
                        compositional reasoning.</p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">

            <div class="columns is-centered has-text-centered">

                <!-- Framework. -->
                <div class="column is-max-desktop">
                    <div class="content has-text-justified">
                        <h2 class="title is-3">Framework</h2>
                        <img src="./static/genemi.png" class="interpolation-image" alt="Generative-Negative-Mining" />
                        <p>
                            <strong>Left:</strong> The portion to the left of the red dotted line demonstrates the process for determining
                            segmentation masks of all objects in the scene.
                            Tag2Text model is first utilized to generate a
                            list of tags for all objects in the scene.
                            Segmentation masks from the source image are created for all the
                            individual tags (Masks for the seagull and water tags are shown).
                            Note that the human-annotated source caption may not contain all
                            the identified tags. Therefore, the caption generated by Tag2Text
                            for the source image is also utilized. <br><br>
                            
                            <strong>Right:</strong> The portion to the right of the
                            red dotted line figure corresponds to the process of
                            generating images having subtle variations from the 
                            source image. For this, we use the Stable Diffusion,
                            the inpainting mode generates the variations by using
                            segmentation mask and new item portrayal as prompt.
                            The item portrayals are produced using ChatGPT.
                        </p>
                        

                    </div>
                </div>
                <!--/ Framework. -->
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
    
            <div class="columns is-centered has-text-centered">
    
                <!-- Samples. -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Samples</h2>
                        <p>
                            We showcase a few examples from our generated validation set.
                            Our approach is advantageous in that we can
                            generate a diverse dataset with challenging negative examples.
                            For instance, the images in the first row depict scenarios
                            that are highly unlikely in the real world, since an ice
                            cream cart will never appear at an airport for aircraft maintenance.
                            These examples serve as a true test of the model’s
                            understanding of the cart concept.
                        </p>
                        <img src="./static/validation_samples.png" class="interpolation-image" alt="Generative-Negative-Mining" />

                    </div>
                </div>
                <!--/ Samples. -->
    
            </div>
            <!--/ Matting. -->

    
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre style="width: 105%"><code>
@inproceedings{sahin2024enhancing,
    title={Enhancing multimodal compositional reasoning of visual language models with generative negative mining},
    author={Sahin, Ugur and Li, Hang and Khan, Qadeer and Cremers, Daniel and Tresp, Volker},
    booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
    pages={5563--5573},
    year={2024}
}
</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/ugorsahin/Generative-Negative-Mining" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            This page is created by using <a
                                href="https://github.com/nerfies/nerfies.github.io">this source code</a> of Nerfies website.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>